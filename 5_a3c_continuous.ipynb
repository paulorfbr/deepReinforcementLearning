{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulorfbr/deepReinforcementLearning/blob/a3c/5_a3c_continuous.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ichf8nc2rKTE"
      },
      "source": [
        "Asynchronous, Advantage Actor-Critic (A3C) agent training script\n",
        "Chapter 3, TensorFlow 2 Reinforcement Learning Cookbook | Praveen Palanisamy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aHnhI_sIrKTI"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "from datetime import datetime\n",
        "from multiprocessing import cpu_count\n",
        "from threading import Thread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "esKngViYrKTK"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, Lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nq6Y8GsrKTL",
        "outputId": "00b05d3b-b783-4680-c1e2-0a412d7c239e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.set_floatx(\"float64\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8AGOFYhrKTL",
        "outputId": "a26afdf9-7dbe-42cc-96ee-89b81a7df8e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--logdir'], dest='logdir', nargs=None, const=None, default='logs', type=None, choices=None, required=False, help=None, metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "parser = argparse.ArgumentParser(prog=\"TFRL-Cookbook-Ch3-A3C\")\n",
        "parser.add_argument(\"--env\", default=\"MountainCarContinuous-v0\")\n",
        "parser.add_argument(\"--num-workers\", default=2, type=int)\n",
        "parser.add_argument(\"--actor-lr\", type=float, default=0.001)\n",
        "parser.add_argument(\"--critic-lr\", type=float, default=0.002)\n",
        "parser.add_argument(\"--update-interval\", type=int, default=5)\n",
        "parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
        "parser.add_argument(\"--logdir\", default=\"logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8NUBbiUrKTN",
        "outputId": "b123080e-4140-4522-a2c6-bd2f87fc8f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving training logs to:logs/TFRL-Cookbook-Ch3-A3C/MountainCarContinuous-v0/20240404-151645\n"
          ]
        }
      ],
      "source": [
        "args = parser.parse_args([])\n",
        "logdir = os.path.join(\n",
        "    args.logdir, parser.prog, args.env, datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        ")\n",
        "print(f\"Saving training logs to:{logdir}\")\n",
        "writer = tf.summary.create_file_writer(logdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MVf1FzKLrKTO"
      },
      "outputs": [],
      "source": [
        "GLOBAL_EPISODE_NUM = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Vn0RijAPrKTO"
      },
      "outputs": [],
      "source": [
        "class Actor:\n",
        "    def __init__(self, state_dim, action_dim, action_bound, std_bound):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_bound = action_bound\n",
        "        self.std_bound = std_bound\n",
        "        self.model = self.nn_model()\n",
        "        self.opt = tf.keras.optimizers.Adam(args.actor_lr)\n",
        "        self.entropy_beta = 0.01\n",
        "\n",
        "    def nn_model(self):\n",
        "        state_input = Input((self.state_dim,))\n",
        "        dense_1 = Dense(32, activation=\"relu\")(state_input)\n",
        "        dense_2 = Dense(32, activation=\"relu\")(dense_1)\n",
        "        out_mu = Dense(self.action_dim, activation=\"tanh\")(dense_2)\n",
        "        mu_output = Lambda(lambda x: x * self.action_bound)(out_mu)\n",
        "        std_output = Dense(self.action_dim, activation=\"softplus\")(dense_2)\n",
        "        return tf.keras.models.Model(state_input, [mu_output, std_output])\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = np.reshape(state, [1, self.state_dim])\n",
        "        mu, std = self.model.predict(state)\n",
        "        mu, std = mu[0], std[0]\n",
        "        return np.random.normal(mu, std, size=self.action_dim)\n",
        "\n",
        "    def log_pdf(self, mu, std, action):\n",
        "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
        "        var = std ** 2\n",
        "        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(\n",
        "            var * 2 * np.pi\n",
        "        )\n",
        "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
        "\n",
        "    def compute_loss(self, mu, std, actions, advantages):\n",
        "        log_policy_pdf = self.log_pdf(mu, std, actions)\n",
        "        loss_policy = log_policy_pdf * advantages\n",
        "        return tf.reduce_sum(-loss_policy)\n",
        "\n",
        "    def train(self, states, actions, advantages):\n",
        "        with tf.GradientTape() as tape:\n",
        "            mu, std = self.model(states, training=True)\n",
        "            loss = self.compute_loss(mu, std, actions, advantages)\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZiWT50FhrKTP"
      },
      "outputs": [],
      "source": [
        "class Critic:\n",
        "    def __init__(self, state_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.model = self.nn_model()\n",
        "        self.opt = tf.keras.optimizers.Adam(args.critic_lr)\n",
        "\n",
        "    def nn_model(self):\n",
        "        return tf.keras.Sequential(\n",
        "            [\n",
        "                Input((self.state_dim,)),\n",
        "                Dense(32, activation=\"relu\"),\n",
        "                Dense(32, activation=\"relu\"),\n",
        "                Dense(16, activation=\"relu\"),\n",
        "                Dense(1, activation=\"linear\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def compute_loss(self, v_pred, td_targets):\n",
        "        mse = tf.keras.losses.MeanSquaredError()\n",
        "        return mse(td_targets, v_pred)\n",
        "\n",
        "    def train(self, states, td_targets):\n",
        "        with tf.GradientTape() as tape:\n",
        "            v_pred = self.model(states, training=True)\n",
        "            # assert (\n",
        "            #    v_pred.shape == td_targets.shape\n",
        "            # ), f\"{v_pred.shape} not equal {td_targets.shape}\"\n",
        "\n",
        "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aJcnK0OQrKTQ"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, env_name, num_workers=cpu_count()):\n",
        "        env = gym.make(env_name)\n",
        "        self.env_name = env_name\n",
        "        self.state_dim = env.observation_space.shape[0]\n",
        "        self.action_dim = env.action_space.shape[0]\n",
        "        self.action_bound = env.action_space.high[0]\n",
        "        self.std_bound = [1e-2, 1.0]\n",
        "\n",
        "        self.global_actor = Actor(\n",
        "            self.state_dim, self.action_dim, self.action_bound, self.std_bound\n",
        "        )\n",
        "        self.global_critic = Critic(self.state_dim)\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def train(self, max_episodes=1000):\n",
        "        workers = []\n",
        "\n",
        "        for i in range(self.num_workers):\n",
        "            env = gym.make(self.env_name)\n",
        "            workers.append(\n",
        "                A3CWorker(env, self.global_actor, self.global_critic, max_episodes)\n",
        "            )\n",
        "\n",
        "        for worker in workers:\n",
        "            worker.start()\n",
        "\n",
        "        for worker in workers:\n",
        "            worker.join()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o-JypZ29rKTR"
      },
      "outputs": [],
      "source": [
        "class A3CWorker(Thread):\n",
        "    def __init__(self, env, global_actor, global_critic, max_episodes):\n",
        "        Thread.__init__(self)\n",
        "        self.env = env\n",
        "        self.state_dim = self.env.observation_space.shape[0]\n",
        "        self.action_dim = self.env.action_space.shape[0]\n",
        "        self.action_bound = self.env.action_space.high[0]\n",
        "        self.std_bound = [1e-2, 1.0]\n",
        "\n",
        "        self.max_episodes = max_episodes\n",
        "        self.global_actor = global_actor\n",
        "        self.global_critic = global_critic\n",
        "        self.actor = Actor(\n",
        "            self.state_dim, self.action_dim, self.action_bound, self.std_bound\n",
        "        )\n",
        "        self.critic = Critic(self.state_dim)\n",
        "\n",
        "        self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
        "        self.critic.model.set_weights(self.global_critic.model.get_weights())\n",
        "\n",
        "    def n_step_td_target(self, rewards, next_v_value, done):\n",
        "        td_targets = np.zeros_like(rewards)\n",
        "        cumulative = 0\n",
        "        if not done:\n",
        "            cumulative = next_v_value\n",
        "\n",
        "        for k in reversed(range(0, len(rewards))):\n",
        "            cumulative = args.gamma * cumulative + rewards[k]\n",
        "            td_targets[k] = cumulative\n",
        "        return td_targets\n",
        "\n",
        "    def advantage(self, td_targets, baselines):\n",
        "        return td_targets - baselines\n",
        "\n",
        "    def train(self):\n",
        "        global GLOBAL_EPISODE_NUM\n",
        "        while self.max_episodes >= GLOBAL_EPISODE_NUM:\n",
        "            state_batch = []\n",
        "            action_batch = []\n",
        "            reward_batch = []\n",
        "            episode_reward, done = 0, False\n",
        "\n",
        "            state = self.env.reset()\n",
        "\n",
        "            while not done:\n",
        "                # self.env.render()\n",
        "                action = self.actor.get_action(state)\n",
        "                action = np.clip(action, -self.action_bound, self.action_bound)\n",
        "\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "                state = np.reshape(state, [1, self.state_dim])\n",
        "                action = np.reshape(action, [1, 1])\n",
        "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
        "                reward = np.reshape(reward, [1, 1])\n",
        "                state_batch.append(state)\n",
        "                action_batch.append(action)\n",
        "                reward_batch.append(reward)\n",
        "\n",
        "                if len(state_batch) >= args.update_interval or done:\n",
        "                    states = np.array([state.squeeze() for state in state_batch])\n",
        "                    actions = np.array([action.squeeze() for action in action_batch])\n",
        "                    rewards = np.array([reward.squeeze() for reward in reward_batch])\n",
        "                    next_v_value = self.critic.model.predict(next_state)\n",
        "                    td_targets = self.n_step_td_target(rewards, next_v_value, done)\n",
        "                    advantages = td_targets - self.critic.model.predict(states)\n",
        "\n",
        "                    actor_loss = self.global_actor.train(states, actions, advantages)\n",
        "                    critic_loss = self.global_critic.train(states, td_targets)\n",
        "\n",
        "                    self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
        "                    self.critic.model.set_weights(\n",
        "                        self.global_critic.model.get_weights()\n",
        "                    )\n",
        "\n",
        "                    state_batch = []\n",
        "                    action_batch = []\n",
        "                    reward_batch = []\n",
        "\n",
        "                episode_reward += reward[0][0]\n",
        "                state = next_state[0]\n",
        "\n",
        "            print(f\"Episode#{GLOBAL_EPISODE_NUM} Reward:{episode_reward}\")\n",
        "            tf.summary.scalar(\"episode_reward\", episode_reward, step=GLOBAL_EPISODE_NUM)\n",
        "            GLOBAL_EPISODE_NUM += 1\n",
        "\n",
        "    def run(self):\n",
        "        self.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm_UpdnfrKTS",
        "outputId": "d0b53b4f-6625-48d1-b514-137702a93c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 3s 3s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 196ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 203ms/step\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 179ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 0s 200ms/step\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "1/1 [==============================] - 0s 319ms/step\n",
            "1/1 [==============================] - 0s 380ms/step\n",
            "1/1 [==============================] - 0s 479ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-5cd991fa6542>:29: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  td_targets[k] = cumulative\n",
            "WARNING:tensorflow:5 out of the last 21 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7da8a06ef7f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 486ms/step\n",
            "1/1 [==============================] - 0s 245ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Exception in thread Thread-12:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"<ipython-input-10-5cd991fa6542>\", line 88, in run\n",
            "  File \"<ipython-input-10-5cd991fa6542>\", line 68, in train\n",
            "Thread-11:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "  File \"<ipython-input-7-785caab0cb18>\", line 43, in train\n",
            "    self.run()\n",
            "  File \"<ipython-input-10-5cd991fa6542>\", line 88, in run\n",
            "  File \"<ipython-input-10-5cd991fa6542>\", line 68, in train\n",
            "  File \"<ipython-input-7-785caab0cb18>\", line 43, in train\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\", line 1066, in gradient\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\", line 1066, in gradient\n",
            "    flat_grad = imperative_grad.imperative_grad(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/imperative_grad.py\", line 67, in imperative_grad\n",
            "    return pywrap_tfe.TFE_Py_TapeGradient(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\", line 148, in _gradient_function\n",
            "    flat_grad = imperative_grad.imperative_grad(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/imperative_grad.py\", line 67, in imperative_grad\n",
            "    return pywrap_tfe.TFE_Py_TapeGradient(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\", line 148, in _gradient_function\n",
            "    return grad_fn(mock_op, *out_grads)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_grad.py\", line 1402, in _SubGrad\n",
            "    return _ReduceGradientArgs(x, y, gx, gy)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_grad.py\", line 144, in _ReduceGradientArgs\n",
            "    gy = _ReduceGradientArg(gy, by)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_grad.py\", line 135, in _ReduceGradientArg\n",
            "    grad = array_ops.reshape(grad, shape)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\", line 88, in wrapper\n",
            "    return grad_fn(mock_op, *out_grads)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_grad.py\", line 1402, in _SubGrad\n",
            "    return op(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
            "        return _ReduceGradientArgs(x, y, gx, gy)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_grad.py\", line 143, in _ReduceGradientArgs\n",
            "    gx = _ReduceGradientArg(gx, bx)\n",
            "raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 5883, in raise_from_not_ok_status\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_grad.py\", line 134, in _ReduceGradientArg\n",
            "    grad = math_ops.reduce_sum(grad, axes, keepdims=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\", line 88, in wrapper\n",
            "    return op(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\", line 5883, in raise_from_not_ok_status\n",
            "    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Sum_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 25 values, but the requested shape has 5 [Op:Sum] name: \n",
            "    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 25 values, but the requested shape has 5 [Op:Reshape] name: \n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7da8a042e9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7da8a042e9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env_name = \"MountainCarContinuous-v0\"\n",
        "    agent = Agent(env_name, args.num_workers)\n",
        "    agent.train(max_episodes=10)  # Increase max_episodes value"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "executable": "/usr/bin/env python",
      "formats": "ipynb,py"
    },
    "kernelspec": {
      "display_name": "tfrl-cookbook",
      "language": "python",
      "name": "tfrl-cookbook"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}